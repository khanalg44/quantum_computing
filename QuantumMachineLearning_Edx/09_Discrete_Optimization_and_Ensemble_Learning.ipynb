{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any learning algorithm will always have strengths and weaknesses: a single model is unlikely to fit every possible scenario. Ensembles combine multiple models to achieve higher generalization performance than any of the constituent models is capable of. How do we assemble the weak learners? We can use some sequential heuristics. For instance, given the current collection of models, we can add one more based on where that particular model performs well. Alternatively, we can look at all the correlations of the predictions between all models, and optimize for the most uncorrelated predictors. Since this latter is a global approach, it naturally maps to a quantum computer. But first, let's take a look a closer look at loss functions and regularization, two key concepts in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and Regularization\n",
    "\n",
    "If you can solve a problem by a classical computer -- let that be a laptop or a massive GPU cluster -- there is little value in solving it by a quantum computer that costs ten million dollars. The interesting question in quantum machine learning is whether there are problems in machine learning and AI that fit quantum computers naturally, but are challenging on classical hardware. This, however, requires a good understanding of both machine learning and contemporary quantum computers.\n",
    "\n",
    "In this course, we primarily focus on the second aspect, since there is no shortage of educational material on classical machine learning. However, it is worth spending a few minutes on going through some basics.\n",
    "\n",
    "Let us take a look at the easiest possible problem: the data points split into two, easily distinguishable sets. We randomly generate this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "c1 = np.random.rand(50, 2)/5\n",
    "c2 = (-0.6, 0.5) + np.random.rand(50, 2)/5\n",
    "data = np.concatenate((c1, c2))\n",
    "labels = np.array([0] * 50 + [1] *50)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(111, xticks=[], yticks=[])\n",
    "plt.scatter(data[:50, 0], data[:50, 1], color='navy')\n",
    "plt.scatter(data[50:, 0], data[50:, 1], color='c');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the data set into a training set that we are going to optimize over (2/3 of the data), and a test set where we estimate our generalization performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(labels))\n",
    "np.random.shuffle(idx)\n",
    "# train on a random 2/3 and test on the remaining 1/3\n",
    "idx_train = idx[:2*len(idx)//3]\n",
    "idx_test = idx[2*len(idx)//3:]\n",
    "X_train = data[idx_train]\n",
    "X_test = data[idx_test]\n",
    "y_train = labels[idx_train]\n",
    "y_test = labels[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the package `scikit-learn` to train various machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "metric = sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a perceptron, which has a linear loss function $\\frac{1}{N}\\sum_{i=1}^N |h(x_i)-y_i)|$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  1.00\n",
      "accuracy (test):  1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model_1 = Perceptron()\n",
    "model_1.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_1.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does a great job. It is a linear model, meaning its decision surface is a plane. Our dataset is separable by a plane, so let's try another linear model, but this time a support vector machine. If you eyeball our dataset, you will see that to define the separation between the two classes, actually only a few points close to the margin are relevant. These are called support vectors and support vector machines aim to find them. Its objective function measures the loss and it has a regularization term with a weight $C$. The $C$ hyperparameter controls a regularization term that penalizes the objective for the number of support vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  1.00\n",
      "accuracy (test):  1.00\n",
      "Number of support vectors: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_2 = SVC(kernel='linear', C=1)\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))\n",
    "print('Number of support vectors:', sum(model_2.n_support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It picks only two datapoints out of the hundred. Let's change the hyperparameter to reduce the penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.52\n",
      "accuracy (test):  0.47\n",
      "Number of support vectors: 64\n"
     ]
    }
   ],
   "source": [
    "model_2 = SVC(kernel='linear', C=0.01)\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))\n",
    "print('Number of support vectors:', sum(model_2.n_support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the model gets confused by using two many datapoints in the final classifier. This is one example where regularization helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "Ensembles yield better results when there is considerable diversity among the base classifiers. If diversity is sufficient, base classifiers make different errors, and a strategic combination may reduce the total error, ideally improving generalization performance. A constituent model in an ensemble is also called a base classifier or weak learner, and the composite model a strong learner.\n",
    "\n",
    "The generic procedure of ensemble methods has two steps. First, develop a set of base classifiers from the training data. Second, combine them to form the ensemble. In the simplest combination, the base learners vote, and the label prediction is based on majority. More involved methods weigh the votes of the base learners. \n",
    "\n",
    "Let us import some packages and define our figure of merit as accuracy in a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.000793Z",
     "start_time": "2018-11-19T20:10:17.128450Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "%matplotlib inline\n",
    "\n",
    "metric = sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a random dataset of two classes that form concentric circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.174692Z",
     "start_time": "2018-11-19T20:10:18.003641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "data, labels = sklearn.datasets.make_circles()\n",
    "idx = np.arange(len(labels))\n",
    "np.random.shuffle(idx)\n",
    "# train on a random 2/3 and test on the remaining 1/3\n",
    "idx_train = idx[:2*len(idx)//3]\n",
    "idx_test = idx[2*len(idx)//3:]\n",
    "X_train = data[idx_train]\n",
    "X_test = data[idx_test]\n",
    "\n",
    "y_train = 2 * labels[idx_train] - 1  # binary -> spin\n",
    "y_test = 2 * labels[idx_test] - 1\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "normalizer = sklearn.preprocessing.Normalizer()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_test = normalizer.fit_transform(X_test)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.subplot(111, xticks=[], yticks=[])\n",
    "plt.scatter(data[labels == 0, 0], data[labels == 0, 1], color='navy')\n",
    "plt.scatter(data[labels == 1, 0], data[labels == 1, 1], color='c');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.226327Z",
     "start_time": "2018-11-19T20:10:18.177561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.44\n",
      "accuracy (test):  0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "model_1 = Perceptron()\n",
    "model_1.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_1.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_1.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its decision surface is linear, we get a poor accuracy. Would a support vector machine with a nonlinear kernel fare better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.244639Z",
     "start_time": "2018-11-19T20:10:18.230025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.62\n",
      "accuracy (test):  0.24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_2 = SVC(kernel='rbf')\n",
    "model_2.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_2.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_2.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs better on the training set, but at the cost of extremely poor generalization. \n",
    "\n",
    "Boosting is an ensemble method that explicitly seeks models that complement one another. The variation between boosting algorithms is how they combine weak learners. Adaptive boosting (AdaBoost) is a popular method that combines the weak learners in a sequential manner based on their individual accuracies. It has a convex objective function that does not penalize for complexity: it is likely to include all available weak learners in the final ensemble. Let's train AdaBoost with a few weak learners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.314089Z",
     "start_time": "2018-11-19T20:10:18.248869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.65\n",
      "accuracy (test):  0.29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model_3 = AdaBoostClassifier(n_estimators=3)\n",
    "model_3.fit(X_train, y_train)\n",
    "print('accuracy (train): %5.2f'%(metric(y_train, model_3.predict(X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, model_3.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its performance is marginally better than that of the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QBoost\n",
    "\n",
    "The idea of Qboost is that optimization on a quantum computer is not constrained to convex objective functions, therefore we can add arbitrary penalty terms and rephrase our objective [[1](#1)]. Qboost solves the following problem:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\sum_{k=1}^{K}w_kh_k(x_i)-\n",
    "y_i\\right)^2+\\lambda\\|w\\|_0\\right),\n",
    "$$\n",
    "\n",
    "where $h_k(x_i)$ is the prediction of the weak learner $k$ for a training instance $k$. The weights in this formulation are binary, so this objective function is already maps to an Ising model. The regularization in the $l_0$ norm ensures sparsity, and it is not the kind of regularization we would consider classically: it is hard to optimize with this term on a digital computer.\n",
    "\n",
    "Let us expand the quadratic part of the objective:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\\frac{1}{N}\\sum_{i=1}^{N}\n",
    "\\left( \\left(\\sum_{k=1}^{K} w_k h_k(x_i)\\right)^{2} -\n",
    "2\\sum_{k=1}^{K} w_k h_k(\\mathbf{x}_i)y_i + y_i^{2}\\right) + \\lambda \\|w\\|_{0}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Since $y_i^{2}$ is just a constant offset, the optimization reduces to\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{w} \\left(\n",
    "\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{l=1}^{K} w_k w_l\n",
    "\\left(\\sum_{i=1}^{N}h_k(x_i)h_l(x_i)\\right) - \n",
    "\\frac{2}{N}\\sum_{k=1}^{K}w_k\\sum_{i=1}^{N} h_k(x_i)y_i +\n",
    "\\lambda \\|w\\|_{0} \\right).\n",
    "$$\n",
    "\n",
    "This form shows that we consider all correlations between the predictions of the weak learners: there is a summation of $h_k(x_i)h_l(x_i)$. Since this term has a positive sign, we penalize for correlations. On the other hand, the correlation with the true label, $h_k(x_i)y_i$, has a negative sign. The regularization term remains unchanged.\n",
    "\n",
    "Let us consider all three models from the previous section as weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.320974Z",
     "start_time": "2018-11-19T20:10:18.316633Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate their predictions and set $\\lambda$ to 1. The predictions are scaled to reflecting the averaging in the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.354723Z",
     "start_time": "2018-11-19T20:10:18.323802Z"
    }
   },
   "outputs": [],
   "source": [
    "n_models = len(models)\n",
    "\n",
    "predictions = np.array([h.predict(X_train) for h in models], dtype=np.float64)\n",
    "# scale hij to [-1/N, 1/N]\n",
    "predictions *= 1/n_models\n",
    "\n",
    "λ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the quadratic binary optimization of the objective function as we expanded above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.375760Z",
     "start_time": "2018-11-19T20:10:18.357248Z"
    }
   },
   "outputs": [],
   "source": [
    "w = np.dot(predictions, predictions.T)\n",
    "wii = len(X_train) / (n_models ** 2) + λ - 2 * np.dot(predictions, y_train)\n",
    "w[np.diag_indices_from(w)] = wii\n",
    "W = {}\n",
    "for i in range(n_models):\n",
    "    for j in range(i, n_models):\n",
    "        W[(i, j)] = w[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the quadratic binary optimization with simulated annealing and read out the optimal weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.703378Z",
     "start_time": "2018-11-19T20:10:18.378217Z"
    }
   },
   "outputs": [],
   "source": [
    "import dimod\n",
    "sampler = dimod.SimulatedAnnealingSampler()\n",
    "response = sampler.sample_qubo(W, num_reads=10)\n",
    "weights = list(response.first.sample.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a prediction function to help with measuring accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.715360Z",
     "start_time": "2018-11-19T20:10:18.705496Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(models, weights, X):\n",
    "\n",
    "    n_data = len(X)\n",
    "    T = 0\n",
    "    y = np.zeros(n_data)\n",
    "    for i, h in enumerate(models):\n",
    "        y0 = weights[i] * h.predict(X)  # prediction of weak classifier\n",
    "        y += y0\n",
    "        T += np.sum(y0)\n",
    "\n",
    "    y = np.sign(y - T / (n_data*len(models)))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.734604Z",
     "start_time": "2018-11-19T20:10:18.719931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (train):  0.65\n",
      "accuracy (test):  0.29\n"
     ]
    }
   ],
   "source": [
    "print('accuracy (train): %5.2f'%(metric(y_train, predict(models, weights, X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, predict(models, weights, X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy co-incides with our strongest weak learner's, the AdaBoost model. Looking at the optimal weights, this is apparent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.751765Z",
     "start_time": "2018-11-19T20:10:18.736771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only AdaBoost made it to the final ensemble. The first two models perform poorly and their predictions are correlated. Yet, if you remove regularization by setting $\\lambda=0$ above, the second model also enters the ensemble, decreasing overall performance. This shows that the regularization is in fact important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving by QAOA\n",
    "\n",
    "Since eventually our problem is just an Ising model, we can also solve it on a gate-model quantum computer by QAOA. Let us explicitly map the binary optimization to the Ising model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:18.765328Z",
     "start_time": "2018-11-19T20:10:18.754605Z"
    }
   },
   "outputs": [],
   "source": [
    "h, J, offset = dimod.qubo_to_ising(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to translate the Ising couplings to be suitable for solving by the QAOA routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:19.838597Z",
     "start_time": "2018-11-19T20:10:18.767740Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qiskit_aqua'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7b8ce47d6c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mqiskit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_info\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPauli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mqiskit_aqua\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpauli_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qiskit_aqua'"
     ]
    }
   ],
   "source": [
    "from qiskit.quantum_info import Pauli\n",
    "from qiskit_aqua import Operator\n",
    "\n",
    "num_nodes = w.shape[0]\n",
    "pauli_list = []\n",
    "for i in range(num_nodes):\n",
    "    wp = np.zeros(num_nodes)\n",
    "    vp = np.zeros(num_nodes)\n",
    "    vp[i] = 1\n",
    "    pauli_list.append([h[i], Pauli(vp, wp)])\n",
    "    for j in range(i+1, num_nodes):\n",
    "        if w[i, j] != 0:\n",
    "            wp = np.zeros(num_nodes)\n",
    "            vp = np.zeros(num_nodes)\n",
    "            vp[i] = 1\n",
    "            vp[j] = 1\n",
    "            pauli_list.append([J[i, j], Pauli(vp, wp)])\n",
    "ising_model = Operator(paulis=pauli_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:40.568546Z",
     "start_time": "2018-11-19T20:10:19.840830Z"
    }
   },
   "outputs": [],
   "source": [
    "from qiskit_aqua import get_aer_backend, QuantumInstance\n",
    "from qiskit_aqua.algorithms import QAOA\n",
    "from qiskit_aqua.components.optimizers import COBYLA\n",
    "p = 1\n",
    "optimizer = COBYLA()\n",
    "qaoa = QAOA(ising_model, optimizer, p, operator_mode='matrix')\n",
    "backend = get_aer_backend('statevector_simulator')\n",
    "quantum_instance = QuantumInstance(backend, shots=100)\n",
    "result = qaoa.run(quantum_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we extract the most likely solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:40.577140Z",
     "start_time": "2018-11-19T20:10:40.571807Z"
    }
   },
   "outputs": [],
   "source": [
    "k = np.argmax(result['eigvecs'][0])\n",
    "weights = np.zeros(num_nodes)\n",
    "for i in range(num_nodes):\n",
    "    weights[i] = k % 2\n",
    "    k >>= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the weights found by QAOA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:40.597309Z",
     "start_time": "2018-11-19T20:10:40.579449Z"
    }
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:10:40.614781Z",
     "start_time": "2018-11-19T20:10:40.602793Z"
    }
   },
   "outputs": [],
   "source": [
    "print('accuracy (train): %5.2f'%(metric(y_train, predict(models, weights, X_train))))\n",
    "print('accuracy (test): %5.2f'%(metric(y_test, predict(models, weights, X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Neven, H., Denchev, V.S., Rose, G., Macready, W.G. (2008). [Training a binary classifier with the quantum adiabatic algorithm](https://arxiv.org/abs/0811.0416). *arXiv:0811.0416*.  <a id='1'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
